import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.impute import KNNImputer
from sklearn.feature_selection import SelectKBest, f_regression, RFE
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

# Paso 1: Generar el Dataset Sintético
# Datos iniciales
data = {
    'Name': ['John', 'Alice', 'Bob', 'Emma', 'Tom'],
    'Age': [20, np.nan, 22, 19, 18],
    'TestScore': [85, 90, np.nan, 78, 92],
    'Grade': ['A', 'B', np.nan, 'C', 'A']
}
df_initial = pd.DataFrame(data)

# Generar un dataset con 1000 muestras y 15 características
np.random.seed(42)
X = np.random.randn(1000, 15)
y = 2 * np.sin(X[:, 0]) + np.log(np.abs(X[:, 1]) + 1) + X[:, 2] ** 2 + 3 * X[:, 3] + np.sqrt(
    np.abs(X[:, 4])) + np.random.randn(1000) * 0.1

# Convertir a DataFrame para facilitar la manipulación
columns = [f"feature_{i}" for i in range(15)]
df = pd.DataFrame(X, columns=columns)
df['target'] = y

# Introducir valores faltantes aleatorios en el 30% de las filas para cada característica
for col in df.columns:
    df.loc[df.sample(frac=0.3).index, col] = np.nan

# Paso 2: Imputación Avanzada
# Imputación por regresión para Age e Income (feature_0 y feature_1)
imputer_reg = IterativeImputer(random_state=42)
df[['feature_0', 'feature_1']] = imputer_reg.fit_transform(df[['feature_0', 'feature_1']])

# Imputación con KNN para TestScore y SpendingScore (feature_2 y feature_3)
imputer_knn = KNNImputer(n_neighbors=5)
df[['feature_2', 'feature_3']] = imputer_knn.fit_transform(df[['feature_2', 'feature_3']])

# Imputación para el resto de las características
other_features = df.columns.difference(['feature_0', 'feature_1', 'feature_2', 'feature_3', 'target'])
df[other_features] = imputer_knn.fit_transform(df[other_features])

# Verificar si aún hay NaN
print("NaN después de la imputación:\n", df.isna().sum())

# Paso 3: Crear características no lineales (polinomiales y logarítmicas)
# Asegurarse de que no haya valores NaN antes de aplicar la transformación polinomial
df = df.dropna()  # Eliminar cualquier fila que aún pueda tener NaN si es necesario

poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(df.drop(columns='target'))
X_poly_df = pd.DataFrame(X_poly, columns=[f"poly_{i}" for i in range(X_poly.shape[1])])

# Concatenar con las características originales
df = pd.concat([df.reset_index(drop=True), X_poly_df.reset_index(drop=True)], axis=1)
print("Dataset con características no lineales:\n", df.head())

# Paso 4: Selección de Características
X = df.drop(columns='target')
y = df['target']

# Selección univariada con SelectKBest
select_kbest = SelectKBest(score_func=f_regression, k=10)
X_selected_kbest = select_kbest.fit_transform(X, y)

# Selección multivariada con RFE (Regresión Lineal)
model = LinearRegression()
rfe = RFE(estimator=model, n_features_to_select=10)
X_selected_rfe = rfe.fit_transform(X, y)

# Paso 5: Modelado de Regresión Lineal Polinomial
X_train, X_test, y_train, y_test = train_test_split(X_selected_rfe, y, test_size=0.3, random_state=42)

# Crear modelo de regresión lineal polinomial
poly_model = Pipeline([('poly', PolynomialFeatures(degree=2)),
                       ('linear', LinearRegression())])

# Entrenar el modelo
poly_model.fit(X_train, y_train)

# Predecir en el conjunto de prueba
y_pred = poly_model.predict(X_test)

# Evaluar el modelo
mse = mean_squared_error(y_test, y_pred)
print(f'Error Cuadrático Medio (MSE): {mse}')

# Paso 6: Normalización
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Entrenar el modelo con datos normalizados
poly_model.fit(X_train_scaled, y_train)
y_pred_scaled = poly_model.predict(X_test_scaled)

# Evaluar el modelo con normalización
mse_scaled = mean_squared_error(y_test, y_pred_scaled)
print(f'Error Cuadrático Medio con Normalización (MSE): {mse_scaled}')

# Paso 7: Visualización de diferentes regresiones polinomiales (grados 1, 2 y 3)
X_vis = df['feature_0'].values.reshape(-1, 1)
y_vis = df['target'].values


# Función para graficar regresiones polinomiales
def plot_polynomial_regression(X_vis, y_vis, degrees):
    X_plot = np.linspace(X_vis.min(), X_vis.max(), 100).reshape(-1, 1)
    plt.scatter(X_vis, y_vis, color='black', label='Datos originales')

    for degree in degrees:
        poly = PolynomialFeatures(degree=degree)
        X_poly = poly.fit_transform(X_vis)

        model = LinearRegression()
        model.fit(X_poly, y_vis)

        X_plot_poly = poly.transform(X_plot)
        y_plot = model.predict(X_plot_poly)

        plt.plot(X_plot, y_plot, label=f'Grado {degree}')

    plt.xlabel('feature_0')
    plt.ylabel('target')
    plt.title('Regresiones Polinomiales')
    plt.legend()
    plt.show()


# Graficar regresiones polinomiales con grados 1, 2 y 3
plot_polynomial_regression(X_vis, y_vis, degrees=[1, 2, 3])
